{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create Preprocessed.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmCxuEEkEJis",
        "outputId": "205ac89b-adba-446f-ef7a-07c2da554def"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%load_ext google.colab.data_table\n",
        "import bz2\n",
        "from functools import partial\n",
        "from collections import Counter, OrderedDict\n",
        "import pickle\n",
        "import heapq\n",
        "from itertools import islice, count, groupby\n",
        "from xml.etree import ElementTree\n",
        "import codecs\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import gzip\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "from time import time\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtvQ4gKZE1vh",
        "outputId": "7c6826b2-70d0-42c0-950d-9f3317e1bef6"
      },
      "source": [
        "try:\n",
        "    import mwparserfromhell as mwp\n",
        "except ImportError:\n",
        "    !pip install -I --compile mwparserfromhell==0.6.0\n",
        "finally:\n",
        "    import mwparserfromhell as mwp\n",
        "mwp.definitions.INVISIBLE_TAGS.append('ref')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mwparserfromhell==0.6.0\n",
            "  Downloading mwparserfromhell-0.6-cp37-cp37m-manylinux1_x86_64.whl (174 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174 kB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: mwparserfromhell\n",
            "Successfully installed mwparserfromhell-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8WWXAKVEP8n",
        "outputId": "bc50cf68-bb63-4ae4-d2af-df819a17146d"
      },
      "source": [
        "## Download one wikipedia file\n",
        "part_url = 'https://dumps.wikimedia.org/enwiki/20210801/enwiki-20210801-pages-articles-multistream27.xml-p66975910p68380251.bz2'\n",
        "wiki_file = Path(part_url).name\n",
        "!wget -N $part_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-20 15:59:17--  https://dumps.wikimedia.org/enwiki/20210801/enwiki-20210801-pages-articles-multistream27.xml-p66975910p68380251.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7, 2620:0:861:1:208:80:154:7\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 368265417 (351M) [application/octet-stream]\n",
            "Saving to: ‘enwiki-20210801-pages-articles-multistream27.xml-p66975910p68380251.bz2’\n",
            "\n",
            "enwiki-20210801-pag 100%[===================>] 351.21M  4.27MB/s    in 79s     \n",
            "\n",
            "2021-10-20 16:00:36 (4.47 MB/s) - ‘enwiki-20210801-pages-articles-multistream27.xml-p66975910p68380251.bz2’ saved [368265417/368265417]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzVO95ObE6dt"
      },
      "source": [
        "def page_iter(wiki_file):\n",
        "  \"\"\" Reads a wiki dump file and create a generator that yields one page at a \n",
        "      time. \n",
        "  Parameters:\n",
        "  -----------\n",
        "  wiki_file: str\n",
        "    A path to wiki dump file.\n",
        "  Returns:\n",
        "  --------\n",
        "  tuple\n",
        "    containing three elements: article id, title, and body. \n",
        "  \"\"\"\n",
        "  # open compressed bz2 dump file\n",
        "  with bz2.open(wiki_file, 'rt', encoding='utf-8', errors='ignore') as f_in:\n",
        "    # Create iterator for xml that yields output when tag closes\n",
        "    elems = (elem for _, elem in ElementTree.iterparse(f_in, events=(\"end\",)))\n",
        "    # Consume the first element and extract the xml namespace from it. \n",
        "    # Although the raw xml has the  short tag names without namespace, i.e. it \n",
        "    # has <page> tags and not <http://wwww.mediawiki.org/xml/export...:page> \n",
        "    # tags, the parser reads it *with* the namespace. Therefore, it needs the \n",
        "    # namespace when looking for child elements in the find function as below.\n",
        "    elem = next(elems)\n",
        "    m = re.match(\"^{(http://www\\.mediawiki\\.org/xml/export-.*?)}\", elem.tag)\n",
        "    if m is None:\n",
        "        raise ValueError(\"Malformed MediaWiki dump\")\n",
        "    ns = {\"ns\": m.group(1)}\n",
        "    page_tag = ElementTree.QName(ns['ns'], 'page').text\n",
        "    # iterate over elements\n",
        "    for elem in elems:\n",
        "      if elem.tag == page_tag:\n",
        "        # Filter out redirect and non-article pages\n",
        "        if elem.find('./ns:redirect', ns) is not None or \\\n",
        "           elem.find('./ns:ns', ns).text != '0':\n",
        "          elem.clear()\n",
        "          continue\n",
        "        # Extract the article wiki id\n",
        "        wiki_id = int(elem.find('./ns:id', ns).text)\n",
        "        # Extract the article title into a variables called title\n",
        "        title = elem.find('./ns:title', ns).text\n",
        "        # extract body\n",
        "        body = elem.find('./ns:revision/ns:text', ns).text\n",
        "\n",
        "        yield wiki_id, title, body\n",
        "        elem.clear()\n",
        "\n",
        "def pages_iter(wiki_file, batch_size=1000):\n",
        "  \"\"\" Generator that yields multiple wiki pages in a batch. Yields the batch \n",
        "      index (0, 1, ..) and an iterable of pages with `batch_size` elements. \n",
        "      This function is designed to handle batches read directly from the xml file.\n",
        "  \"\"\"\n",
        "  for i, group in groupby(enumerate(page_iter(wiki_file)), \n",
        "                          lambda x: x[0] // batch_size):\n",
        "    _, batch = zip(*group)\n",
        "    yield i, batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1LevZioFBAa"
      },
      "source": [
        "### CREATE MAPPING OF ARTICLE TITLE TO WIKI ID\n",
        "def get_title2wid():\n",
        "  # If file already exists, load it.\n",
        "  if os.path.exists('title2wid.pkl'):\n",
        "    with open('title2wid.pkl', 'rb') as f:\n",
        "      return pickle.load(f)\n",
        "  # Otherwise, create mapping from scratch \n",
        "  RE_wid_title = re.compile(r\"\\((\\d+),0,'(.+?)(?<!\\\\)','',(0|1)\", re.UNICODE)\n",
        "  # Download wiki pages database\n",
        "  !wget -N https://dumps.wikimedia.org/enwiki/20210801/enwiki-20210801-page.sql.gz\n",
        "  wid2title = {}\n",
        "  title2wid = {}\n",
        "  re_title2wid = {}\n",
        "  # wid2re_title = {}\n",
        "  with gzip.open('enwiki-20210801-page.sql.gz', \"rt\", encoding='utf-8', errors='ignore') as f:\n",
        "    for line in f:\n",
        "      if not line.startswith(\"INSERT INTO\"):\n",
        "        continue\n",
        "      for m in RE_wid_title.finditer(line):\n",
        "        wid = int(m.group(1))\n",
        "        if m.group(3) == '0':\n",
        "          wid2title[wid] = m.group(2).lower()\n",
        "          title2wid[m.group(2).lower()] = wid\n",
        "        else:\n",
        "          re_title2wid[m.group(2).lower()] = wid\n",
        "  # Download wiki page redirect database\n",
        "  !wget -N https://dumps.wikimedia.org/enwiki/20210801/enwiki-20210801-redirect.sql.gz\n",
        "  RE_redirects = re.compile(r\"\\((\\d+),0,'(.+?)(?<!\\\\)'\", re.UNICODE)\n",
        "  wid2target = {}\n",
        "  with gzip.open('enwiki-20210801-redirect.sql.gz', \"rt\", encoding='utf-8', errors='ignore') as f:\n",
        "    for line in f:\n",
        "      if not line.startswith(\"INSERT INTO\"):\n",
        "        continue\n",
        "      for m in RE_redirects.finditer(line):\n",
        "        wid = int(m.group(1))\n",
        "        wid2target[wid] = m.group(2).lower()\n",
        "  # follow redirects\n",
        "  for re_title, re_id in re_title2wid.items():\n",
        "    target = wid2target.get(re_id, None)\n",
        "    if target is None:\n",
        "      continue\n",
        "    target_id = title2wid.get(target, None)\n",
        "    if target_id is None:\n",
        "      continue\n",
        "    title2wid[re_title] = target_id\n",
        "\n",
        "  with open('title2wid.pkl', 'wb') as f:\n",
        "    pickle.dump(title2wid, f)\n",
        "  return title2wid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqWVreQPFFTp"
      },
      "source": [
        "title2wid = get_title2wid()\n",
        "RE_FORBIDDEN_CHARS = re.compile(r\"[#\\<\\>\\[\\]\\{\\}\\|]\", re.UNICODE)\n",
        "def get_wiki_id(title):\n",
        "  \"\"\" Return the Wikipedia article id for a given article title or None \n",
        "      otherwise.\n",
        "  \"\"\"  \n",
        "  t_new = title.lower().replace(' ', '_').replace(\"'\", \"\\\\'\")\n",
        "  t_new = RE_FORBIDDEN_CHARS.sub('', t_new)\n",
        "  return title2wid.get(t_new, None)\n",
        "\n",
        "RE_NON_ARTICLE = re.compile(\n",
        "  r'(#|:|{|([fF]ile|[iI]mage|[mM]edia|[sS]pecial|[cC]ategory):)', \n",
        "  re.UNICODE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCSbKiaSFJFc"
      },
      "source": [
        "def get_wikilinks(wikicode):\n",
        "  \"\"\" Traverses the parse tree for internal links and filter out non-article \n",
        "      links.\n",
        "  Parameters:\n",
        "  -----------\n",
        "  wikicode: mwp.wikicode.Wikicode\n",
        "    Parse tree of some WikiMedia markdown.\n",
        "  Returns:\n",
        "  --------\n",
        "  list of (target_id: int, anchor_text: str) pairs\n",
        "    A list of outgoing links from the markdown to wikipedia articles.\n",
        "  \"\"\"\n",
        "  links = []\n",
        "  for wl in wikicode.ifilter_wikilinks():\n",
        "    # skip links that don't pass our filter\n",
        "    title = str(wl.title)\n",
        "    if RE_NON_ARTICLE.match(title):\n",
        "      continue\n",
        "    # remove any lingering section/anchor reference in the link\n",
        "    title = title.split('#')[0]\n",
        "    # Get article id from title\n",
        "    target_id = get_wiki_id(title)\n",
        "    if target_id is None:\n",
        "      continue\n",
        "    # if text is None use title, otherwise strip markdown from the anchor text.\n",
        "    text = wl.text\n",
        "    if text is None:\n",
        "      text = title\n",
        "    else:\n",
        "      text = text.strip_code()\n",
        "    links_tuple = {'id': target_id, 'text': text} \n",
        "    links.append(links_tuple)\n",
        "    # links.append((target_id, text))\n",
        "\n",
        "  return links\n",
        "\n",
        "def parse_mediawiki(body):\n",
        "  wikicode = mwp.parse(body, skip_style_tags=True)\n",
        "  links = get_wikilinks(wikicode)\n",
        "  return wikicode.strip_code(), links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmgesSk13M1g"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'core-period-321814'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaKo6Fu3JIM2"
      },
      "source": [
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "gcs_service = build('storage', 'v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDFr3ssQFMJb"
      },
      "source": [
        "# Create preprocessed file\n",
        "def upload_to_gcp(file_name, service = gcs_service):\n",
        "  media = MediaFileUpload(file_name, resumable=True)\n",
        "\n",
        "  request = gcs_service.objects().insert(bucket='wikidata_preprocessed', \n",
        "                                        name=file_name,\n",
        "                                        media_body=media)\n",
        "\n",
        "  response = None\n",
        "  while response is None:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "    _, response = request.next_chunk()\n",
        "\n",
        "  print('Upload complete')\n",
        "  \n",
        "def create_preprocessed(wiki_file):\n",
        "  # file_name = wiki_file.split(\".\")[0].split(\"-\")[-1] +\"_part2_preprocessed.parquet\"\n",
        "  file_name = wiki_file.split(\".\")[0].split(\"-\")[-1] +\"_part3_preprocessed.parquet\"\n",
        "  wiki_ids = []\n",
        "  titles = []\n",
        "  bodies = []\n",
        "  lists_of_links = []\n",
        "  counter = 0\n",
        "  for wiki_id, title, body in page_iter(wiki_file):\n",
        "    body, links = parse_mediawiki(body)\n",
        "    wiki_ids.append(wiki_id)\n",
        "    titles.append(title)\n",
        "    bodies.append(body)\n",
        "    lists_of_links.append(links)\n",
        "\n",
        "  df = pd.DataFrame({\"id\":wiki_ids, \n",
        "                    \"title\": titles, \n",
        "                    \"text\": bodies, \n",
        "                    \"anchor_text\":lists_of_links})\n",
        "  print (f\"writing {file_name} to parquet file\")\n",
        "  df.to_parquet(file_name)\n",
        "  print (f\"uploading {file_name} to storage\")\n",
        "  upload_to_gcp(file_name)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aScaHguA6sMt",
        "outputId": "472c0c00-1abd-45f7-a9db-4ed154f0ff9d"
      },
      "source": [
        "create_preprocessed(wiki_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "writing multistream27_part3_preprocessed.parquet to parquet file\n",
            "uploading multistream27_part3_preprocessed.parquet to storage\n",
            "Upload complete\n"
          ]
        }
      ]
    }
  ]
}